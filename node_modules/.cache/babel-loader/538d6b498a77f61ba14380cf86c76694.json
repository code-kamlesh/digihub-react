{"ast":null,"code":"import { EndOfStreamError } from 'peek-readable';\nimport { AbstractTokenizer } from './AbstractTokenizer.js';\nexport class BufferTokenizer extends AbstractTokenizer {\n  /**\r\n   * Construct BufferTokenizer\r\n   * @param uint8Array - Uint8Array to tokenize\r\n   * @param fileInfo - Pass additional file information to the tokenizer\r\n   */\n  constructor(uint8Array, fileInfo) {\n    super(fileInfo);\n    this.uint8Array = uint8Array;\n    this.fileInfo.size = this.fileInfo.size ? this.fileInfo.size : uint8Array.length;\n  }\n  /**\r\n   * Read buffer from tokenizer\r\n   * @param uint8Array - Uint8Array to tokenize\r\n   * @param options - Read behaviour options\r\n   * @returns {Promise<number>}\r\n   */\n\n\n  async readBuffer(uint8Array, options) {\n    if (options && options.position) {\n      if (options.position < this.position) {\n        throw new Error('`options.position` must be equal or greater than `tokenizer.position`');\n      }\n\n      this.position = options.position;\n    }\n\n    const bytesRead = await this.peekBuffer(uint8Array, options);\n    this.position += bytesRead;\n    return bytesRead;\n  }\n  /**\r\n   * Peek (read ahead) buffer from tokenizer\r\n   * @param uint8Array\r\n   * @param options - Read behaviour options\r\n   * @returns {Promise<number>}\r\n   */\n\n\n  async peekBuffer(uint8Array, options) {\n    const normOptions = this.normalizeOptions(uint8Array, options);\n    const bytes2read = Math.min(this.uint8Array.length - normOptions.position, normOptions.length);\n\n    if (!normOptions.mayBeLess && bytes2read < normOptions.length) {\n      throw new EndOfStreamError();\n    } else {\n      uint8Array.set(this.uint8Array.subarray(normOptions.position, normOptions.position + bytes2read), normOptions.offset);\n      return bytes2read;\n    }\n  }\n\n  async close() {// empty\n  }\n\n}","map":{"version":3,"sources":["D:/Tata_Strive/WebApplication/digiHub/trunk/digiHubWeb/node_modules/strtok3/lib/BufferTokenizer.js"],"names":["EndOfStreamError","AbstractTokenizer","BufferTokenizer","constructor","uint8Array","fileInfo","size","length","readBuffer","options","position","Error","bytesRead","peekBuffer","normOptions","normalizeOptions","bytes2read","Math","min","mayBeLess","set","subarray","offset","close"],"mappings":"AAAA,SAASA,gBAAT,QAAiC,eAAjC;AACA,SAASC,iBAAT,QAAkC,wBAAlC;AACA,OAAO,MAAMC,eAAN,SAA8BD,iBAA9B,CAAgD;AACnD;AACJ;AACA;AACA;AACA;AACIE,EAAAA,WAAW,CAACC,UAAD,EAAaC,QAAb,EAAuB;AAC9B,UAAMA,QAAN;AACA,SAAKD,UAAL,GAAkBA,UAAlB;AACA,SAAKC,QAAL,CAAcC,IAAd,GAAqB,KAAKD,QAAL,CAAcC,IAAd,GAAqB,KAAKD,QAAL,CAAcC,IAAnC,GAA0CF,UAAU,CAACG,MAA1E;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;;;AACoB,QAAVC,UAAU,CAACJ,UAAD,EAAaK,OAAb,EAAsB;AAClC,QAAIA,OAAO,IAAIA,OAAO,CAACC,QAAvB,EAAiC;AAC7B,UAAID,OAAO,CAACC,QAAR,GAAmB,KAAKA,QAA5B,EAAsC;AAClC,cAAM,IAAIC,KAAJ,CAAU,uEAAV,CAAN;AACH;;AACD,WAAKD,QAAL,GAAgBD,OAAO,CAACC,QAAxB;AACH;;AACD,UAAME,SAAS,GAAG,MAAM,KAAKC,UAAL,CAAgBT,UAAhB,EAA4BK,OAA5B,CAAxB;AACA,SAAKC,QAAL,IAAiBE,SAAjB;AACA,WAAOA,SAAP;AACH;AACD;AACJ;AACA;AACA;AACA;AACA;;;AACoB,QAAVC,UAAU,CAACT,UAAD,EAAaK,OAAb,EAAsB;AAClC,UAAMK,WAAW,GAAG,KAAKC,gBAAL,CAAsBX,UAAtB,EAAkCK,OAAlC,CAApB;AACA,UAAMO,UAAU,GAAGC,IAAI,CAACC,GAAL,CAAS,KAAKd,UAAL,CAAgBG,MAAhB,GAAyBO,WAAW,CAACJ,QAA9C,EAAwDI,WAAW,CAACP,MAApE,CAAnB;;AACA,QAAK,CAACO,WAAW,CAACK,SAAd,IAA4BH,UAAU,GAAGF,WAAW,CAACP,MAAzD,EAAiE;AAC7D,YAAM,IAAIP,gBAAJ,EAAN;AACH,KAFD,MAGK;AACDI,MAAAA,UAAU,CAACgB,GAAX,CAAe,KAAKhB,UAAL,CAAgBiB,QAAhB,CAAyBP,WAAW,CAACJ,QAArC,EAA+CI,WAAW,CAACJ,QAAZ,GAAuBM,UAAtE,CAAf,EAAkGF,WAAW,CAACQ,MAA9G;AACA,aAAON,UAAP;AACH;AACJ;;AACU,QAALO,KAAK,GAAG,CACV;AACH;;AA/CkD","sourcesContent":["import { EndOfStreamError } from 'peek-readable';\r\nimport { AbstractTokenizer } from './AbstractTokenizer.js';\r\nexport class BufferTokenizer extends AbstractTokenizer {\r\n    /**\r\n     * Construct BufferTokenizer\r\n     * @param uint8Array - Uint8Array to tokenize\r\n     * @param fileInfo - Pass additional file information to the tokenizer\r\n     */\r\n    constructor(uint8Array, fileInfo) {\r\n        super(fileInfo);\r\n        this.uint8Array = uint8Array;\r\n        this.fileInfo.size = this.fileInfo.size ? this.fileInfo.size : uint8Array.length;\r\n    }\r\n    /**\r\n     * Read buffer from tokenizer\r\n     * @param uint8Array - Uint8Array to tokenize\r\n     * @param options - Read behaviour options\r\n     * @returns {Promise<number>}\r\n     */\r\n    async readBuffer(uint8Array, options) {\r\n        if (options && options.position) {\r\n            if (options.position < this.position) {\r\n                throw new Error('`options.position` must be equal or greater than `tokenizer.position`');\r\n            }\r\n            this.position = options.position;\r\n        }\r\n        const bytesRead = await this.peekBuffer(uint8Array, options);\r\n        this.position += bytesRead;\r\n        return bytesRead;\r\n    }\r\n    /**\r\n     * Peek (read ahead) buffer from tokenizer\r\n     * @param uint8Array\r\n     * @param options - Read behaviour options\r\n     * @returns {Promise<number>}\r\n     */\r\n    async peekBuffer(uint8Array, options) {\r\n        const normOptions = this.normalizeOptions(uint8Array, options);\r\n        const bytes2read = Math.min(this.uint8Array.length - normOptions.position, normOptions.length);\r\n        if ((!normOptions.mayBeLess) && bytes2read < normOptions.length) {\r\n            throw new EndOfStreamError();\r\n        }\r\n        else {\r\n            uint8Array.set(this.uint8Array.subarray(normOptions.position, normOptions.position + bytes2read), normOptions.offset);\r\n            return bytes2read;\r\n        }\r\n    }\r\n    async close() {\r\n        // empty\r\n    }\r\n}\r\n"]},"metadata":{},"sourceType":"module"}